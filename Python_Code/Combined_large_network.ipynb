{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"9bxNvYbK1voe"},"outputs":[],"source":["#load dependecies\n","from google.colab import drive\n","drive.mount('/content/drive/')\n","\n","import tensorflow as tf\n","from tensorflow.keras import regularizers\n","import keras as k\n","\n","import matplotlib.pyplot as plt\n","import torchsummary as summary\n","import numpy as np\n","np.random.seed(53702)\n","import pandas as pd\n","import os\n","\n","!pip install shap --quiet\n","import shap\n","%matplotlib inline\n","\n","!pip install scikit-survival --quiet\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn import preprocessing\n","\n","from keras.models import Sequential, Model\n","from keras.layers import Dense, Dropout, Activation, Flatten, Input, concatenate, Conv1D,Conv2D,AveragePooling2D,MaxPooling2D,LSTM\n","from keras.backend import dropout \n","from keras.layers.merge import concatenate\n","from tensorflow.keras.utils import plot_model\n","\n","!pip install lifelines --quiet\n","import lifelines\n","\n","from sklearn.experimental import enable_iterative_imputer\n","from sklearn.impute import IterativeImputer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KZt4CN6FH7hN"},"outputs":[],"source":["def loss_lik_efron(y_true,y_pred):\n","  #defining loss function\n","  time = y_true[:,0]\n","  event = y_true[:,1]\n","\n","  y_pred= k.backend.flatten(y_pred)\n","  y_pred=tf.cast(y_pred,tf.float32)\n","\n","  n = tf.shape(time)[0]\n","  sort_index=tf.nn.top_k(time,k=n,sorted=True).indices\n","\n","  time = k.backend.gather(reference=time,indices = sort_index)\n","  event = k.backend.gather(reference=event,indices = sort_index)  \n","  y_pred = k.backend.gather(reference=y_pred,indices = sort_index)  \n","\n","  time_event = time * event\n","  unique_ftime = tf.unique(tf.boolean_mask(tensor = time_event, mask = tf.greater(time_event, 0))).y\n","  m = tf.shape(unique_ftime)[0]\n","  tie_count=tf.unique_with_counts(tf.boolean_mask(time_event, tf.greater(time_event, 0))).count\n","  ind_matrix = k.backend.expand_dims(time,0) - k.backend.expand_dims(time,1)\n","  ind_matrix = k.backend.equal(x=ind_matrix,y=k.backend.zeros_like(ind_matrix))\n","\n","  time_count = k.backend.cumsum(tf.unique_with_counts(time).count)\n","  time_count = k.backend.cast(time_count - k.backend.ones_like(time_count), dtype = tf.int32)\n","  ind_matrix = k.backend.gather(ind_matrix, time_count)\n","  ind_matrix=tf.cast(ind_matrix,'float32')\n","\n","\n","  event=tf.cast(event,'float32')\n","  tie_haz = k.backend.exp(y_pred) * event\n","  tie_haz = k.backend.dot(ind_matrix, k.backend.expand_dims(tie_haz))\n","  event_index = tf.math.not_equal(tie_haz,0)\n","  tie_haz = tf.boolean_mask(tie_haz, event_index)\n","\n","  tie_risk = y_pred * event\n","  tie_risk = k.backend.dot(ind_matrix, k.backend.expand_dims(tie_risk))\n","  tie_risk = tf.boolean_mask(tie_risk, event_index)\n","\n","  cum_haz = k.backend.dot(ind_matrix, k.backend.expand_dims(k.backend.exp(y_pred)))\n","  cum_haz = k.backend.reverse(tf.cumsum(k.backend.reverse(cum_haz, axes = 1)), axes = 1)\n","  cum_haz = tf.boolean_mask(cum_haz, event_index)\n","\n","  global likelihood\n","\n","  if likelihood is None:\n","    likelihood = tf.Variable(0., trainable = False) \n","\n","  j = tf.cast(0, dtype = tf.int32)\n","  def loop_cond(j,a,b,c,d,e):\n","    return j < m\n","\n","  def loop_body(j, tc, tr, th, ch, lik):\n","    l = tc[j]\n","    l = k.backend.cast(l, dtype = tf.float32)\n","    J = tf.linspace(start = tf.cast(0, tf.float32), stop = l-1, num = tf.cast(l, tf.int32))/l \n","    Dm = ch[j] - J*th[j]\n","    lik = lik + tr[j] - tf.math.reduce_sum(tf.math.log(Dm))\n","    one = k.backend.ones_like(j)\n","    j_new = j + one\n","    return(list([j_new, tc, tr, th, ch, lik]))\n","\n","  loop_out = tf.while_loop(cond = loop_cond, body = loop_body,\n","                            loop_vars = list([j, tie_count, tie_risk, tie_haz, cum_haz, likelihood]))\n","  log_lik = loop_out[-1]\n","\n","  return(tf.negative(log_lik))\n","likelihood = None\n","\n","def load_data():\n","  #This function loads the data, filters out sections that have nans, or other \n","  #missing values, and structures them so they can be used in the model afterwards\n","  os.chdir('/content/drive/MyDrive/Speciale/DL after 14-06/Combined')\n","  print(os.listdir())\n","  index_train=pd.read_csv(\"indices_train.csv\",header=None)\n","  index_test=pd.read_csv(\"indices_test.csv\",header=None)\n","\n","  cens_lab = pd.read_csv(\"Comb_cens.txt\")\n","\n","  cens_lab = cens_lab[[\"days\", \"isdead\"]]\n","\n","  os.chdir('/content/drive/MyDrive/Speciale/DL after 14-06/Combined/')\n","    \n","  features_EOG_L=pd.read_csv('Combined_EOG_L.txt')\n","  features_EOG_R=pd.read_csv('Combined_EOG_R.txt')\n","  features_EEG=pd.read_csv('Combined_EEG.txt') \n","  features_EMG=pd.read_csv('Combined_EMG.txt')\n","  features_Time=pd.read_csv('CombFeats.txt')\n","\n","  imp = IterativeImputer(max_iter=10, random_state=0)\n","  IterativeImputer(random_state=0)\n","\n","  os.chdir('/content/drive/MyDrive/Speciale/DL after 14-06/Combined_Reduced/')\n","  CombConf = pd.read_csv('CombConf' +\".txt\")\n","\n","  imp.fit(CombConf)\n","  Imputed=imp.transform(CombConf)\n","  Comb_Conf_confounders=pd.DataFrame(Imputed,columns=CombConf.columns)\n","\n","  Comb_t_feats = pd.read_csv('CombFeats' +\".txt\")\n","\n","  Comb_Conf=pd.concat([CombConf['Age'],Comb_t_feats],1)\n","\n","  Time_features_org= pd.read_csv('CombFeats' +\".txt\") \n","\n","  df=Comb_Conf_confounders\n","  df.Diabetes = df.Diabetes.round()\n","  df.Smoke_status = df.Smoke_status.round()\n","  df.Coffee = df.Coffee.round()\n","  df.Education = df.Education.round()\n","  df.Race = df.Race.round()\n","\n","  cens_lab = pd.read_csv(\"Comb_cens.txt\")\n","\n","  combcens = pd.concat([cens_lab,df],1)\n","\n","  TmpTimeFeat=Time_features_org.copy()\n","\n","  df1=df.copy()\n","\n","  features_Time=pd.read_csv('CombFeats.txt')\n","\n","  features_Time=pd.concat([df1,features_Time],1)\n","\n","  ###\n","  np.random.seed(53702)\n","  randnums= np.random.randint(0,10000,len(cens_lab))/10000\n","  features_EOG_L[\"RandomArray\"]=randnums\n","  features_EOG_R[\"RandomArray2\"]=randnums\n","  features_EEG[\"RandomArray3\"]=randnums\n","  features_EMG[\"RandomArray5\"]=randnums\n","  ### \n","  print(np.shape(features_EOG_L))\n","\n","  rows_with_nan = [index for index, row in features_EOG_L.iterrows() if row.isnull().any()]\n","\n","  print(str(len(rows_with_nan)) + ' removed for having nans as features')\n","\n","  features_EOG_L=features_EOG_L.drop(axis=0, index=rows_with_nan)\n","  features_EOG_R=features_EOG_R.drop(axis=0, index=rows_with_nan)\n","  features_EEG=features_EEG.drop(axis=0, index=rows_with_nan)\n","  features_EMG=features_EMG.drop(axis=0, index=rows_with_nan)\n","  features_Time=features_Time.drop(axis=0, index=rows_with_nan)\n","\n","  cens_lab=cens_lab.drop(axis=0,index=rows_with_nan)\n","\n","  Rem_more = [index for index, row in cens_lab.iterrows() if row.isnull().any()]\n","\n","  print(str(len(Rem_more)) + ' removed for having empty censoring label')\n","\n","  features_EOG_L=features_EOG_L.drop(axis=0, index=Rem_more)\n","  features_EOG_R=features_EOG_R.drop(axis=0, index=Rem_more)\n","  features_EEG=features_EEG.drop(axis=0, index=Rem_more)\n","  features_EMG=features_EMG.drop(axis=0, index=Rem_more)\n","  features_Time=features_Time.drop(axis=0, index=Rem_more)\n","\n","  cens_lab=cens_lab.drop(axis=0,index=Rem_more)\n","\n","  cens_lab=cens_lab.astype(int)\n","\n","  np_EOG_L = features_EOG_L.to_numpy()\n","  np_EOG_R = features_EOG_R.to_numpy()\n","  np_EEG = features_EEG.to_numpy()\n","  np_EMG = features_EMG.to_numpy()\n","  np_Time=features_Time.to_numpy()\n","\n","  np_cens=cens_lab.to_numpy()\n","\n","  nptrainidx=index_train.to_numpy().astype(int);\n","  nptestidx=index_test.to_numpy().astype(int);\n","\n","  x_train_EOG_L = np_EOG_L[nptrainidx,:]\n","  x_train_EOG_R = np_EOG_R[nptrainidx,:]\n","  x_train_EEG = np_EEG[nptrainidx,:]\n","  x_train_EMG = np_EMG[nptrainidx,:]\n","  x_train_Time= np_Time[nptrainidx,:]\n","\n","  y_train_tmp=np_cens[nptrainidx,:]\n","\n","  x_test_EOG_L = np_EOG_L[nptestidx,:]\n","  x_test_EOG_R = np_EOG_R[nptestidx,:]\n","  x_test_EEG = np_EEG[nptestidx,:]\n","  x_test_EMG = np_EMG[nptestidx,:]\n","  x_test_Time= np_Time[nptestidx,:]\n","\n","  y_test_tmp=np_cens[nptestidx,:]\n","\n","  x_train_EOG_L=x_train_EOG_L.reshape(np.shape(x_train_EOG_L)[0],np.shape(x_train_EOG_L)[2])\n","  x_train_EOG_R=x_train_EOG_R.reshape(np.shape(x_train_EOG_R)[0],np.shape(x_train_EOG_R)[2])\n","  x_train_EEG=x_train_EEG.reshape(np.shape(x_train_EEG)[0],np.shape(x_train_EEG)[2])\n","  x_train_EMG=x_train_EMG.reshape(np.shape(x_train_EMG)[0],np.shape(x_train_EMG)[2])\n","  x_train_Time=x_train_Time.reshape(np.shape(x_train_Time)[0],np.shape(x_train_Time)[2])\n","\n","\n","  y_train=y_train_tmp.reshape(np.shape(y_train_tmp)[0],np.shape(y_train_tmp)[2])\n","\n","  x_test_EOG_L=x_test_EOG_L.reshape(np.shape(x_test_EOG_L)[0],np.shape(x_test_EOG_L)[2])\n","  x_test_EOG_R=x_test_EOG_R.reshape(np.shape(x_test_EOG_R)[0],np.shape(x_test_EOG_R)[2])\n","  x_test_EEG=x_test_EEG.reshape(np.shape(x_test_EEG)[0],np.shape(x_test_EEG)[2])\n","  x_test_EMG=x_test_EMG.reshape(np.shape(x_test_EMG)[0],np.shape(x_test_EMG)[2])\n","  x_test_Time=x_test_Time.reshape(np.shape(x_test_Time)[0],np.shape(x_test_Time)[2])\n","\n","  y_test=y_test_tmp.reshape(np.shape(y_test_tmp)[0],np.shape(y_test_tmp)[2])\n","\n","  comb_train=np.concatenate([x_train_EOG_L,x_train_EOG_R,x_train_EEG,x_train_EMG],axis=1)\n","  comb_test=np.concatenate([x_test_EOG_L,x_test_EOG_R,x_test_EEG,x_test_EMG],axis=1)\n","\n","  feature_names=list(features_EOG_L.columns)+list(features_EOG_R.columns)+list(features_EEG.columns)+list(features_EMG.columns)\n","  print(np.shape(comb_train))\n","  shapes=[0,x_test_EOG_L.shape[1],x_test_EOG_R.shape[1],x_test_EEG.shape[1],x_test_EMG.shape[1]]\n","\n","  return comb_train,comb_test,feature_names,shapes,y_train,y_test\n","\n","def Init_Model(comb_train,shapes,num_L1,num_lr,dropOut):\n","    \n","    input1 = Input(shape=(np.shape(comb_train)[1],))\n","\n","    def Splitter(tensor,shapes,iter):\n","        \n","        LowerLim=sum(shapes[:iter])\n","        UpperLim=sum(shapes[:iter+1])\n","        Seglength=int((UpperLim-LowerLim-1)/5)\n","        tensor=tensor[:,LowerLim:UpperLim]\n","        \n","        Out_Tensor_REM=tensor[:,0:Seglength]\n","\n","        Out_Tensor_N3=tensor[:,Seglength:Seglength*2]\n","        Out_Tensor_N2=tensor[:,Seglength*2:Seglength*3]\n","        Out_Tensor_N1=tensor[:,Seglength*3:Seglength*4]\n","        Out_Tensor_Wake=tensor[:,Seglength*4:Seglength*5]\n","\n","\n","        Out_Tensor=tf.concat([Out_Tensor_REM, Out_Tensor_N3,Out_Tensor_N2,Out_Tensor_N1,Out_Tensor_Wake], 1)\n","        Dim1=int(Out_Tensor.get_shape()[1]/5)\n","\n","        Out_Tensor=tf.reshape(Out_Tensor, (tf.shape(Out_Tensor_REM)[0],Dim1,5,1))\n","        print('shape')\n","        print(Out_Tensor.get_shape())\n","        return Out_Tensor\n","\n","    def layerdesigner_Dense(input1,shapes,Num):\n","      #Dense layer structure\n","      input = tf.keras.layers.Lambda(lambda x:Splitter(x,shapes,Num))(input1)\n","\n","      input_flattened = Flatten()(input)\n","      #First_Dense_1 = Dense(512,\n","      #                    activation='relu',\n","      #                    kernel_regularizer=regularizers.L1(num_L1))(input_flattened)\n","      #First_Dense_1 = Dropout(dropOut)(First_Dense_1)\n","      First_Dense = Dense(128,\n","                          activation='relu',\n","                          kernel_regularizer=regularizers.L1(num_L1))(input_flattened)\n","      First_Dense = Dropout(dropOut)(First_Dense)\n","      Second_Dense = Dense(64,\n","                          activation='relu',\n","                          kernel_regularizer=regularizers.L1(num_L1))(First_Dense)\n","      Second_Dense = Dropout(dropOut)(Second_Dense)  \n","\n","      Second_Dense = Dense(32,\n","                    activation='relu',\n","                    kernel_regularizer=regularizers.L1(num_L1))(Second_Dense)\n","      Second_Dense = Dropout(dropOut)(Second_Dense) \n","\n","      Second_Dense = Dense(32,\n","                    activation='relu',\n","                    kernel_regularizer=regularizers.L1(num_L1))(Second_Dense)\n","      Second_Dense = Dropout(dropOut)(Second_Dense) \n","\n","      return Second_Dense      \n","    \n","    def layerdesigner_LSTM(input1,shapes,Num):\n","      #LSTM layer\n","      input = tf.keras.layers.Lambda(lambda x:Splitter(x,shapes,Num))(input1)\n","\n","      Shape=input.get_shape()\n","      input_reshaped=tf.squeeze(input,[3])\n","      input_transposed=tf.transpose(input_reshaped,perm=[0,2,1])\n","\n","      First_LSTM = LSTM(1024)(input_transposed)\n","      First_LSTM = Dropout(dropOut)(First_LSTM)\n","\n","      #Second_LSTM = LSTM(32)(First_LSTM)\n","      #Second_LSTM = Dropout(dropOut)(Second_LSTM)  \n","      Dense_1 = Dense(64,\n","                    activation='relu',\n","                    kernel_regularizer=regularizers.L1(num_L1))(First_LSTM)\n","      Dense_1 = Dropout(dropOut)(Dense_1)\n","\n","      Dense_2 = Dense(32,\n","                    activation='relu',\n","                    kernel_regularizer=regularizers.L1(num_L1))(Dense_1)\n","      Dense_2 = Dropout(dropOut)(Dense_2) \n","\n","      return Dense_2    \n","\n","    def layerdesigner_CNN(input1,shapes,Num):\n","      #uncommented code here are other structures that have been tried\n","      input = tf.keras.layers.Lambda(lambda x:Splitter(x,shapes,Num))(input1)\n","\n","      Shape=input.get_shape()\n","      print(Shape)\n","\n","      First_CNN = Conv2D(32,(11,5),padding=\"same\",activation='relu')(input)\n","      Max1=MaxPooling2D((1, 2))(First_CNN)\n","\n","      second_CNN = Conv2D(16,(3,2),padding=\"same\",activation='relu')(Max1)\n","      Max1=MaxPooling2D((1, 2))(second_CNN)\n","\n","      # second_CNN = Conv2D(16,(3,2),padding=\"same\",activation='relu')(Max1)\n","      # Max2=MaxPooling2D((2, 1))(second_CNN)\n","\n","      Flatten_1 = Flatten()(Max1)\n","\n","      #Dense_Out = Dense(128, \n","      #                  activation = 'relu',\n","      #                  kernel_regularizer=regularizers.L1(num_L1))(Flatten_1)\n","\n","      Dense_Out = Dense(32, \n","                        activation = 'relu',\n","                        kernel_regularizer=regularizers.L1(num_L1))(Flatten_1)\n","\n","\n","          # EOG_L_Conv1 = Conv2D(32, (11,5), padding=\"same\", activation='relu',input_shape=([101,5,1]))(EOG_L_input)\n","          # EOG_L_Conv1=MaxPooling2D(pool_size=(2,2))(EOG_L_Conv1)\n","\n","          # EOG_L_Conv2 = Conv2D(64, (3,3), padding=\"same\", activation='relu')(EOG_L_Conv1)\n","          # EOG_L_Conv2=MaxPooling2D(pool_size=(2,2))(EOG_L_Conv2)    \n","\n","          # EOG_L_flattened = Flatten()(EOG_L_Conv2)\n","\n","      return Dense_Out  \n","             \n","    if True:\n","\n","      EOG_L=layerdesigner(input1,shapes,1)\n","      EOG_R=layerdesigner(input1,shapes,2)\n","      EEG=layerdesigner_Dense(input1,shapes,3)\n","      EMG=layerdesigner(input1,shapes,4)\n","    \n","    if False:\n","      EEG=layerdesigner_LSTM(input1,shapes,3)\n","    \n","    if True:\n","      EOG_L=layerdesigner_CNN(input1,shapes,1)\n","      EOG_R=layerdesigner_CNN(input1,shapes,2)\n","      EEG=layerdesigner_CNN(input1,shapes,3)\n","      EMG=layerdesigner_CNN(input1,shapes,4)\n","      Time=layerdesigner_Dense(input1,shapes,5)\n","\n","    \n","    merge=concatenate([EEG,EOG_L,EOG_R,EEG,ECG,EMG])#,Dense_EOG_R_2,Dense_EEG_2])#,Dense_ECG_2,Dense_EMG_2])\n","    merge_dense = Dense(32,\n","                 activation='relu',\n","                 kernel_regularizer=regularizers.L1(num_L1))(merge)\n","    merge_dense = Dropout(dropOut)(merge_dense) \n","    out = Dense(1)(merge)\n","\n","    model = tf.keras.models.Model(inputs = input1, outputs=out)\n","\n","\n","    model.compile(loss=loss_lik_efron,\n","                  optimizer=tf.keras.optimizers.Adam(num_lr),\n","                    metrics=None)\n","    return model\n","\n","def Run_Model(Min_CI,L1s,Lrs,batch_sizes,comb_train,comb_test,feature_names,shapes,model,y_train,y_test):\n","  #this function runs the model, and tests the model using the test set with the concordance index.\n","  #hyperparameters\n","  modelnumber=0\n","  print(\"Model:\" + str(modelnumber) + \"_BS:\" + str(batch_sizes) + \"_LR:\" + str(Lrs) + \"_L1:\" + str(L1s))\n","  num_L1=L1s\n","  num_lr=Lrs\n","  num_epoch=200\n","  batch_size=batch_sizes\n","  \n","  callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=10)\n","\n","  history=model.fit(x=comb_train, y=y_train, \n","              batch_size=batch_size, \n","              epochs=num_epoch, \n","              verbose=1, \n","              validation_split=0.1,\n","              callbacks=[callback])\n","  \n","  predicts=model.predict(comb_test)\n","  df = pd.DataFrame(y_test, columns=['T', 'E'])\n","  df[\"predicts\"]=predicts\n","  if bool(abs(pd.isnull(df[\"predicts\"][1])-1)):\n","    try:\n","      cph = lifelines.fitters.coxph_fitter.CoxPHFitter().fit(df, 'T', 'E')\n","      cindex=round(lifelines.utils.concordance_index(df['T'], -cph.predict_partial_hazard(df), df['E'])*100,1)\n","\n","      print(cindex)\n","\n","      plt.plot(history.history['loss'])\n","      plt.plot(history.history['val_loss'])\n","\n","      modelnumber += 1\n","      modelname=\"Model:\" + str(modelnumber) + \"_BS:\" + str(batch_sizes) + \"_LR:\" + str(Lrs) + \"_L1:\" + str(L1s) + \"_CI:\" + str(cindex)\n","      TitleName=\"Model loss DNNSurv - BS:\" + str(batch_sizes) + \"_LR:\" + str(Lrs) + \"_L1:\" + str(L1s) + \"_CI:\" + str(cindex)\n","      plt.title(TitleName)\n","      plt.ylabel('loss')\n","      plt.xlabel('epoch')\n","      plt.legend(['train', 'test'], loc='upper left')\n","      plt.show()\n","\n","      df.to_csv('Ordering_predicts')\n","    except Exception as e:\n","      print(e) \n","  return model\n","\n","def main(Min_CI,comb_train,comb_test,feature_names,shapes,y_train,y_test):\n","  #hyperparameters\n","  L1s=0.001#[0.1,0.01,0.005,0.001,0.0005,0.0001]\n","  Lrs=0.005#[0.0005]#[1,0.1,0.01,0.005,0.001,0.0005,0.0001]\n","  batch_sizes=256#[32,64,128,256,512,1024]\n","  modelnumber=0\n","  dropOut=0.6\n","\n","  for i in range(1):\n","    print('Iteration: ' + str(i+1))\n","\n","    model=Init_Model(comb_train,shapes,L1s,Lrs,dropOut)\n","    plot_model(model, to_file='multiple_inputs.png',show_shapes=True)\n","    model=Run_Model(Min_CI,L1s,Lrs,batch_sizes,comb_train,comb_test,feature_names,shapes,model,y_train,y_test)\n","    \n","  return model\n","if __name__ == '__main__':\n","    if False:\n","      print('Loading Data...')\n","      comb_train,comb_test,feature_names,shapes,y_train,y_test=load_data()\n","      print('Loading Data Completed')\n","    Min_CI=62.5\n","    model=main(Min_CI,comb_train,comb_test,feature_names,shapes,y_train,y_test)"]}],"metadata":{"colab":{"collapsed_sections":[],"name":"Combined_large_network.ipynb","provenance":[],"authorship_tag":"ABX9TyNSZaRGFN3h6F1MKAUbzxaL"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}