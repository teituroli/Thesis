{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"9bxNvYbK1voe","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1659964821944,"user_tz":-120,"elapsed":24610,"user":{"displayName":"Teitur Óli Kristjánsson","userId":"09453390363522877061"}},"outputId":"28c5d9c8-b0c8-4030-d4eb-0f01f79a8f28"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"]}],"source":["#load dependecies\n","import tensorflow as tf\n","from tensorflow.keras import regularizers\n","import keras as k\n","\n","import matplotlib.pyplot as plt\n","import torchsummary as summary\n","import numpy as np\n","np.random.seed(53702)\n","import pandas as pd\n","import os\n","\n","!pip install shap --quiet\n","import shap\n","%matplotlib inline\n","\n","!pip install scikit-survival --quiet\n","\n","from sklearn.model_selection import train_test_split\n","\n","from keras.models import Sequential\n","from keras.layers import Dense, Dropout, Activation, Flatten, Input\n","from keras.backend import dropout \n","\n","!pip install lifelines --quiet\n","import lifelines\n","\n","#Google colab was used\n","from google.colab import drive\n","drive.mount('/content/drive/')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h4EtMc6ZIixo"},"outputs":[],"source":["def loss_lik_efron(y_true,y_pred):\n","  #define loss function\n","  time = y_true[:,0]\n","  event = y_true[:,1]\n","\n","  y_pred= k.backend.flatten(y_pred)\n","  y_pred=tf.cast(y_pred,tf.float32)\n","\n","  n = tf.shape(time)[0]\n","  sort_index=tf.nn.top_k(time,k=n,sorted=True).indices\n","\n","  time = k.backend.gather(reference=time,indices = sort_index)\n","  event = k.backend.gather(reference=event,indices = sort_index)  \n","  y_pred = k.backend.gather(reference=y_pred,indices = sort_index)  \n","\n","  time_event = time * event\n","  unique_ftime = tf.unique(tf.boolean_mask(tensor = time_event, mask = tf.greater(time_event, 0))).y\n","  m = tf.shape(unique_ftime)[0]\n","  tie_count=tf.unique_with_counts(tf.boolean_mask(time_event, tf.greater(time_event, 0))).count\n","  ind_matrix = k.backend.expand_dims(time,0) - k.backend.expand_dims(time,1)\n","  ind_matrix = k.backend.equal(x=ind_matrix,y=k.backend.zeros_like(ind_matrix))\n","\n","  time_count = k.backend.cumsum(tf.unique_with_counts(time).count)\n","  time_count = k.backend.cast(time_count - k.backend.ones_like(time_count), dtype = tf.int32)\n","  ind_matrix = k.backend.gather(ind_matrix, time_count)\n","  ind_matrix=tf.cast(ind_matrix,'float32')\n","\n","  event=tf.cast(event,'float32')\n","  tie_haz = k.backend.exp(y_pred) * event\n","  tie_haz = k.backend.dot(ind_matrix, k.backend.expand_dims(tie_haz))\n","  event_index = tf.math.not_equal(tie_haz,0)\n","  tie_haz = tf.boolean_mask(tie_haz, event_index)\n","\n","  tie_risk = y_pred * event\n","  tie_risk = k.backend.dot(ind_matrix, k.backend.expand_dims(tie_risk))\n","  tie_risk = tf.boolean_mask(tie_risk, event_index)\n","\n","  cum_haz = k.backend.dot(ind_matrix, k.backend.expand_dims(k.backend.exp(y_pred)))\n","  cum_haz = k.backend.reverse(tf.cumsum(k.backend.reverse(cum_haz, axes = 1)), axes = 1)\n","  cum_haz = tf.boolean_mask(cum_haz, event_index)\n","\n","  global likelihood\n","\n","  if likelihood is None:\n","    likelihood = tf.Variable(0., trainable = False) \n","\n","  j = tf.cast(0, dtype = tf.int32)\n","  def loop_cond(j,a,b,c,d,e):\n","    return j < m\n","\n","  def loop_body(j, tc, tr, th, ch, lik):\n","    l = tc[j]\n","    l = k.backend.cast(l, dtype = tf.float32)\n","    J = tf.linspace(start = tf.cast(0, tf.float32), stop = l-1, num = tf.cast(l, tf.int32))/l \n","    Dm = ch[j] - J*th[j]\n","    lik = lik + tr[j] - tf.math.reduce_sum(tf.math.log(Dm))\n","    one = k.backend.ones_like(j)\n","    j_new = j + one\n","    return(list([j_new, tc, tr, th, ch, lik]))\n","\n","  loop_out = tf.while_loop(cond = loop_cond, body = loop_body,\n","                            loop_vars = list([j, tie_count, tie_risk, tie_haz, cum_haz, likelihood]))\n","  log_lik = loop_out[-1]\n","\n","  return(tf.negative(log_lik))\n","likelihood = None\n","\n","#set hyperparameters\n","L1s=[0.1,0.01,0.005,0.001,0.0005,0.0001]\n","Lrs=[1,0.1,0.01,0.005,0.001,0.0005,0.0001]\n","batch_sizes=[32,64,128,256,512,1024]\n","modelnumber=0\n","\n","#forloop to ensure reproducability\n","for zz in range(1,11):\n","  print(\"Iteration \" + str(zz))\n","\n","  #select which state to investigate\n","  StateToInv='EEG'\n","\n","  #select window length to investigate\n","  WLIteration=zz\n","  os.chdir('/content/drive/MyDrive/Speciale/DL after 14-06/MrOs')\n","\n","  #load train test split (80-20)\n","  index_train=pd.read_csv(\"indices_train.csv\",header=None)\n","  index_test=pd.read_csv(\"indices_test.csv\",header=None)\n","\n","  #load censoring and days label \n","  cens_lab = pd.read_csv(\"MrOs_cens_lab.txt\")\n","\n","  #reorder\n","  cens_lab = cens_lab[[\"days\", \"isdead\"]]\n","\n","  #load band\n","  os.chdir('/content/drive/MyDrive/Speciale/DL after 14-06/MrOs/' + StateToInv)\n","  features = pd.read_csv(StateToInv+\"_WL_\"+ str(WLIteration) +\".txt\")\n","\n","  ## load random number to use as baseline\n","  np.random.seed(53702)\n","  randnums= np.random.randint(0,10000,len(cens_lab))/10000\n","  features[\"RandomArray\"]=randnums\n","  \n","  ### clean signal for nans and other missing values\n","  rows_with_nan = [index for index, row in features.iterrows() if row.isnull().any()]\n","\n","  features=features.drop(axis=0, index=rows_with_nan)\n","  cens_lab=cens_lab.drop(axis=0,index=rows_with_nan)\n","\n","  ###\n","  npfeat=features.to_numpy()\n","  np_cens=cens_lab.to_numpy()\n","  nptrainidx=index_train.to_numpy().astype(int);\n","  nptestidx=index_test.to_numpy().astype(int);\n","\n","  x_train_tmp=npfeat[nptrainidx,:]\n","  y_train_tmp=np_cens[nptrainidx,:]\n","\n","  x_test_tmp=npfeat[nptestidx,:]\n","  y_test_tmp=np_cens[nptestidx,:]\n","\n","  x_train=x_train_tmp.reshape(np.shape(x_train_tmp)[0],np.shape(x_train_tmp)[2])\n","  y_train=y_train_tmp.reshape(np.shape(y_train_tmp)[0],np.shape(y_train_tmp)[2])\n","\n","  x_test=x_test_tmp.reshape(np.shape(x_test_tmp)[0],np.shape(x_test_tmp)[2])\n","  y_test=y_test_tmp.reshape(np.shape(y_test_tmp)[0],np.shape(y_test_tmp)[2])\n","  #A number of nested forloops to test the hyperparameters.\n","  for ss in range(100):\n","    for q in L1s:\n","      for w in Lrs:\n","        for z in batch_sizes:\n","          print(\"Model:\" + str(modelnumber) + \"_BS:\" + str(z) + \"_LR:\" + str(w) + \"_L1:\" + str(q))\n","          num_L1=q\n","          num_lr=w\n","          num_epoch=200\n","          batch_size=z\n","\n","          #This setup here is for the feed forward nerual network to test windowlength. \n","          #A series of different structures were also tested, but this one is just evaluated here. \n","          model = Sequential()\n","\n","          model.add(Dense(128, \n","                          activation='relu',\n","                          kernel_regularizer=regularizers.L1(num_L1)))\n","          model.add(Dropout(0.2))\n","\n","          model.add(Dense(64, \n","                    activation='relu',\n","                    kernel_regularizer=regularizers.L1(num_L1)))\n","          model.add(Dropout(0.2))\n","\n","          model.add(Dense(34, \n","                    activation='relu',\n","                    kernel_regularizer=regularizers.L1(num_L1)))\n","          model.add(Dropout(0.2))\n","\n","          model.add(Dense(1, \n","                          activation='relu'))\n","          \n","          callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=10)\n","\n","          #Compile model and fit it. \n","          model.compile(loss=loss_lik_efron,\n","                  optimizer=tf.keras.optimizers.Adam(num_lr),#tf.keras.optimizers.RMSprop(num_lr),#'adam',\n","                  metrics=None)\n","          \n","          #validation split is set to 10%\n","          history=model.fit(x=x_train, y=y_train, \n","                      batch_size=batch_size, \n","                      epochs=num_epoch, \n","                      verbose=0, \n","                      validation_split=0.1,\n","                      callbacks=[callback])\n","          \n","          #predict using model and test data\n","          predicts=model.predict(x_test)\n","          df = pd.DataFrame(y_test, columns=['T', 'E'])\n","          df[\"predicts\"]=predicts\n","\n","          #make sure it did not fail\n","          if bool(abs(pd.isnull(df[\"predicts\"][1])-1)):\n","            try:\n","              #test using concordance index\n","              cph = lifelines.fitters.coxph_fitter.CoxPHFitter().fit(df, 'T', 'E')\n","              cindex=round(lifelines.utils.concordance_index(df['T'], -cph.predict_partial_hazard(df), df['E'])*100,1)\n","\n","              #plot and save figure\n","              print(cindex)\n","              plt.plot(history.history['loss'])\n","              plt.plot(history.history['val_loss'])\n","\n","              modelnumber += 1\n","              modelname=\"Model:\" + str(modelnumber) + \"_BS:\" + str(z) + \"_LR:\" + str(w) + \"_L1:\" + str(q) + \"_CI:\" + str(cindex)\n","              TitleName=\"Model loss DNNSurv - BS:\" + str(z) + \"_LR:\" + str(w) + \"_L1:\" + str(q) + \"_CI:\" + str(cindex)\n","              plt.title(TitleName)\n","              plt.ylabel('loss')\n","              plt.xlabel('epoch')\n","              plt.legend(['train', 'test'], loc='upper left')\n","              plt.show()\n","              #plt.savefig(modelname+\".png\", bbox_inches='tight')\n","              #plt.clf()\n","            except Exception as e:\n","              print(e)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xEc9HFTcZZAW"},"outputs":[],"source":["#evaluate results using shap (here it is just used as a test)\n","explainer = shap.Explainer(model.predict, x_train,max_evals=2513,feature_names=features.columns.tolist())\n","shap_values = explainer(x_train[:1])\n","\n","#visualise using bar and beeswarm plot\n","pd.set_option('display.max_rows', None)\n","shap.plots.beeswarm(shap_values,max_display=1000)\n","shap.plots.bar(shap_values.abs.mean(0))\n"]}],"metadata":{"colab":{"collapsed_sections":[],"name":"DNNsurv decision of WL length.ipynb","provenance":[],"authorship_tag":"ABX9TyNvfpyRJqGYapuf0iX4GjX/"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}